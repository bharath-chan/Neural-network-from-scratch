{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple representation of a neuron\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    \n",
    "    def activate(self, inputs):\n",
    "        return sum(w * i for w, i in zip(self.weights, inputs)) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions\n",
    "import numpy as np \n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, Y, Z1, A1, Z2, A2, W2):\n",
    "    m = X.shape[1]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient descent\n",
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = np.tanh(self.Z1)\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = self.sigmoid(self.Z2)\n",
    "        return self.A2\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X, Y, iterations, learning_rate):\n",
    "    for i in range(iterations):\n",
    "        A2 = self.forward(X)\n",
    "        cost = self.compute_cost(A2, Y)\n",
    "        dW1, db1, dW2, db2 = self.backward(X, Y, A2)\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "def compute_cost(self, A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X, Y):\n",
    "    # Normalize features\n",
    "    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    Y_encoded = np.eye(np.max(Y) + 1)[Y].T\n",
    "    \n",
    "    return X, Y_encoded\n",
    "\n",
    "# Example usage\n",
    "X_raw = np.random.randn(10, 100)\n",
    "Y_raw = np.random.randint(0, 3, 100)\n",
    "X_processed, Y_processed = preprocess_data(X_raw, Y_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mini-batch gradient descent\n",
    "def create_mini_batches(X, Y, batch_size):\n",
    "    mini_batches = []\n",
    "    data = np.hstack((X, Y))\n",
    "    np.random.shuffle(data)\n",
    "    n_minibatches = data.shape[0] // batch_size\n",
    "    \n",
    "    for i in range(n_minibatches + 1):\n",
    "        mini_batch = data[i * batch_size:(i + 1) * batch_size, :]\n",
    "        X_mini = mini_batch[:, :-1].T\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1)).T\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "    \n",
    "    return mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A2, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    W1, W2 = parameters['W1'], parameters['W2']\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(A2, Y)\n",
    "    L2_regularization_cost = (lambd / (2 * m)) * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    \n",
    "    return cross_entropy_cost + L2_regularization_cost\n",
    "\n",
    "def backward_propagation_with_regularization(X, Y, cache, parameters, lambd):\n",
    "    m = X.shape[1]\n",
    "    W1, W2 = parameters['W1'], parameters['W2']\n",
    "    A1, A2 = cache['A1'], cache['A2']\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T) + (lambd / m) * W2\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T) + (lambd / m) * W1\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_with_dropout(X, parameters, keep_prob):\n",
    "    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    D1 = np.random.rand(A1.shape[0], A1.shape[1]) < keep_prob\n",
    "    A1 = A1 * D1\n",
    "    A1 = A1 / keep_prob\n",
    "    \n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {'Z1': Z1, 'A1': A1, 'D1': D1, 'Z2': Z2, 'A2': A2}\n",
    "    return A2, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning\n",
    "import itertools\n",
    "\n",
    "def grid_search(X, Y, param_grid):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    \n",
    "    param_combinations = list(itertools.product(*param_grid.values()))\n",
    "    \n",
    "    for params in param_combinations:\n",
    "        hidden_size, learning_rate, num_iterations = params\n",
    "        \n",
    "        model = SimpleNeuralNetwork(X.shape[0], hidden_size, Y.shape[0])\n",
    "        model.train(X, Y, num_iterations, learning_rate)\n",
    "        \n",
    "        accuracy = model.evaluate(X, Y)\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params, best_accuracy\n",
    "\n",
    "# Example usage\n",
    "param_grid = {\n",
    "    'hidden_size': [10, 20, 30],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'num_iterations': [1000, 2000, 3000]\n",
    "}\n",
    "\n",
    "best_params, best_accuracy = grid_search(X_train, Y_train, param_grid)\n",
    "print(f\"Best parameters: {best_params}, Best accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Example usage\n",
    "model = SimpleNeuralNetwork(input_size, hidden_size, output_size)\n",
    "model.train(X_train, Y_train, iterations, learning_rate)\n",
    "\n",
    "save_model(model, 'neural_network_model.pkl')\n",
    "\n",
    "loaded_model = load_model('neural_network_model.pkl')\n",
    "predictions = loaded_model.forward(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
